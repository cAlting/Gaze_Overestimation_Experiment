{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124666a6",
   "metadata": {},
   "source": [
    "# 1. Image Centering\n",
    "\n",
    "There are two options for centering:\n",
    "\n",
    "1. Via Googles Mediapipe library\n",
    "A total of 468 facial landmarks are extracted. The images are aligned in the x and y directions so that, after alignment, the center of the line connecting the left and right lacrimal caruncles is located at the center of the image.\n",
    "\n",
    "2. Via dlib (_recommended_)\n",
    "A total of 68 facial landmarks are extracted. The images are aligned in the x and y directions so that, after alignment, the center of the line connecting the left and right lacrimal caruncles is located at the center of the image. For the images used, the alignment result based on the dlib68 landmarks is more convincing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963824a9",
   "metadata": {},
   "source": [
    "### a) Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der relevanten Libraries\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import cv2\n",
    "import imageio.v2 as imageio\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "import mediapipe as mp\n",
    "from mlxtend.image import extract_face_landmarks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965cbd83",
   "metadata": {},
   "source": [
    "### b) Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad80278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root: root directory with subfolders for aligned, cropped, ... images\n",
    "root = r'C:\\Users\\calti\\Documents\\Masterarbeit\\Bilder'\n",
    "\n",
    "# orig_imgs: directory where unedited image files are located\n",
    "imgs_orig = r'C:\\Users\\calti\\Documents\\Masterarbeit\\Bilder\\JPG'\n",
    "\n",
    "# target: directory for aligned images\n",
    "#imgs_aligned = r'C:\\Users\\calti\\Documents\\Masterarbeit\\Bilder\\Mediapipe'\n",
    "imgs_aligned = r'C:\\Users\\calti\\Documents\\Masterarbeit\\Bilder\\mlxtend'\n",
    "\n",
    "# (opt): Directory where the aligned and Photoshop Content Aware Fill processed images are located\n",
    "#imgs_aligned_caf = r'C:\\Users\\calti\\Documents\\Masterarbeit\\Bilder\\Mediapipe CAF'\n",
    "imgs_aligned_caf = r'C:\\Users\\calti\\Documents\\Masterarbeit\\Bilder\\mlxtend CAF'\n",
    "\n",
    "# imgs_cropped: directory for cropped and resized images\n",
    "#imgs_cropped = root+os.sep+'Cropped'\n",
    "imgs_cropped = root + os.sep + imgs_aligned_caf.split(sep='\\\\')[-1] + 'Cropped'\n",
    "\n",
    "# exp_dir: Directory of the PsychoPy experiment in which the conditions file containing the paths to the images should be saved\n",
    "exp = r'C:\\Users\\calti\\Documents\\Masterarbeit\\PsychoPy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4adc7",
   "metadata": {},
   "source": [
    "### c) Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbee94f",
   "metadata": {},
   "source": [
    "#### i) Image centering (Mediapipe Face Mesh 468)\n",
    "\n",
    "Code is partially taken from: [Stackoverflow](https://stackoverflow.com/questions/59525640/how-to-center-the-content-object-of-a-binary-image-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a760fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_image(image, landmark_results, lm1 = 133, lm2 = 362, lm3 = 168, suppress_output = True):\n",
    "    \"\"\"\n",
    "    Centers an image based on the locations of specific facial landmarks detected by a landmark detection model.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    image (ndarray): The image to be centered.\n",
    "    landmark_results (Object): The landmark detection results from a face detection model.\n",
    "    lm1 (int): The index of the first landmark point to be used for centering the image. Default is 133, corresponding to the left eye (around Caruncula lacrimalis).\n",
    "    lm2 (int): The index of the second landmark point to be used for centering the image. Default is 362, corresponding to the right eye (around Caruncula lacrimalis).\n",
    "    lm3 (int): The index of the third landmark point to be used for centering the image. Default is 168, corresponding to a point slightly above the inter-pupillary line.\n",
    "    suppress_output (bool): If True, suppress the output of debugging information to the console. Default is True.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    centered_image (ndarray): The centered image.\n",
    "    ipp_half (float): The x-coordinate of the landmark point in the centered image.\n",
    "    ipp_half_2 (float): The y-coordinate of the landmark point in the centered image.\n",
    "    angle (float): The angle in degrees between the line connecting the left and right eye landmarks and the horizontal axis.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # get image width, height and center coordinates\n",
    "    height, width, chann = image.shape\n",
    "    \n",
    "    # Center of Original Input Image\n",
    "    wi=(width/2)\n",
    "    he=(height/2)\n",
    "\n",
    "    # x, y, und z Koordinaten des relevanten Punktes\n",
    "    # ipp1, ..., enthalten jeweils die x-, y- und z-Koordinaten der Landmark in normalisierter Einheit [0,1]\n",
    "    ipp1 = landmark_results.multi_face_landmarks[0].landmark[lm1] # Auge Links (Caruncula lacrimalis)\n",
    "    ipp2 = landmark_results.multi_face_landmarks[0].landmark[lm2] # Auge rechts (Caruncula lacrimalis)\n",
    "    ipp3 = landmark_results.multi_face_landmarks[0].landmark[lm3] # (Punkt etwas oberhalb der Interpupillarlinie)\n",
    "    \n",
    "    # x und y Koordinaten in Pixeln [0,1] -> Pixel\n",
    "    ipp1_x_px = ipp1.x*width\n",
    "    ipp1_y_px = ipp1.y*height\n",
    "    ipp2_x_px = ipp2.x*width\n",
    "    ipp2_y_px = ipp2.y*height\n",
    "    ipp3_y_px = ipp3.y*height\n",
    "    ipp3_x_px = ipp3.x*width\n",
    "    \n",
    "    # Bestimmung der x-Koordinate des Mittelpunktes zwischen C. lacrimalis rechts und links\n",
    "    # ipp2_x_px: x-Koordinate der C. lacrimalis rechts (landmark 362) [Pixel]\n",
    "    # ipp1_x_px: x-Koordinate der C. lacrimalis links (landmark 133)  [Pixel]\n",
    "    # ipp_half: x-Koordinate der C. lacrimalis links + die Hälfte der Distanz zwischen lm362 und lm133 [Pixel]\n",
    "    #  - dies ist die x-Koordinate des Punktes, der später bei x = Bildbreite/2 liegen soll\n",
    "    ipp_half = ipp1_x_px+(ipp2_x_px-ipp1_x_px)/2\n",
    "    \n",
    "    # Bestimmung der y-Koordinate des Punktes, der später bei y = Bildhöhe/2 liegen soll\n",
    "    if ipp1_y_px < ipp2_y_px:\n",
    "        ipp_half_2 = ipp1_y_px+(ipp2_y_px-ipp1_y_px)/2\n",
    "    else:\n",
    "        ipp_half_2 = ipp2_y_px+(ipp1_y_px-ipp2_y_px)/2\n",
    "    \n",
    "    # Bestimmung des Winkels zwischen der Linie zwischen C.l. sinister und dextra\n",
    "    dX = ipp2_x_px - ipp1_x_px\n",
    "    dY = ipp2_y_px - ipp1_y_px\n",
    "    angle = np.degrees(np.arctan2(dY, dX))\n",
    "    \n",
    "    # Offset = Differenz zwischen (x,y) des Bildzentrums und (x,y) der Landmark\n",
    "    #offsetX = (wi-ipp_half)\n",
    "    #offsetY = (he-ipp_half_2)\n",
    "    offsetX = (wi-ipp3_x_px)\n",
    "    offsetY = (he-ipp_half_2)\n",
    "    \n",
    "    if suppress_output == False:\n",
    "        msg = f'''\n",
    "        EyeL x:   {ipp1_x_px}\n",
    "        EyeR x:   {ipp2_x_px}\n",
    "        EyeL y:   {ipp1_y_px}\n",
    "        EyeR y:   {ipp2_y_px}\n",
    "        IPP x:    {ipp_half}\n",
    "        IPP y:    {ipp_half_2}\n",
    "        Offset x: {offsetX}\n",
    "        Offset y: {offsetY}\n",
    "        Angle:    {angle}\n",
    "        \\n\\n\n",
    "        '''\n",
    "        print(msg)\n",
    "    \n",
    "    # Affine matrix with Translations\n",
    "    T = np.float32([[1, 0, offsetX], [0, 1, offsetY]]) \n",
    "    \n",
    "    # WarpAffine\n",
    "    centered_image = cv2.warpAffine(image, T, (width, height))\n",
    "    \n",
    "    # Return translated Image, x-Koordinate des Punktes zwischen C.l. sinistra und dextra, Winkel\n",
    "    return centered_image, ipp_half, ipp_half_2, angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1545b4-cebe-4b66-a90f-b549f9007dd0",
   "metadata": {},
   "source": [
    "#### ii) Image Centering (mlxtend, dlib68)\n",
    "\n",
    "Code is partially taken from: [Stackoverflow](https://stackoverflow.com/questions/59525640/how-to-center-the-content-object-of-a-binary-image-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854c2ef-5b53-4af5-a29d-328a7daee724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_image_mlx(image, landmarks, lm1 = 39, lm2 = 42, suppress_output = True):\n",
    "    \"\"\"\n",
    "    Centers an image based on the locations of specific facial landmarks detected by a landmark detection model.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The image to center.\n",
    "        landmarks (list): A list of facial landmarks, where each landmark is a\n",
    "            tuple of (x, y) coordinates.\n",
    "        lm1 (int): The index of the first landmark used to calculate the\n",
    "            midpoint. Defaults to 39, which corresponds to the inner corner of\n",
    "            the left eye.\n",
    "        lm2 (int): The index of the second landmark used to calculate the\n",
    "            midpoint. Defaults to 42, which corresponds to the inner corner of\n",
    "            the right eye.\n",
    "        suppress_output (bool): If True, suppresses the output of debugging\n",
    "            information. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    centered_image (ndarray): The centered image.\n",
    "    ipp_half (float): The x-coordinate of the landmark point in the centered image.\n",
    "    ipp_half_2 (float): The y-coordinate of the landmark point in the centered image.\n",
    "    angle (float): The angle in degrees between the line connecting the left and right eye landmarks and the horizontal axis.\n",
    "    \"\"\"\n",
    "    # get image width, height and center coordinates\n",
    "    height, width, chann = image.shape\n",
    "    \n",
    "    # Center of Original Input Image\n",
    "    wi=(width/2)\n",
    "    he=(height/2)\n",
    "\n",
    "    # x, y, und z Koordinaten des relevanten Punktes\n",
    "    # ipp1, ..., enthalten jeweils die x-, y- und z-Koordinaten der Landmark in normalisierter Einheit [0,1]\n",
    "    lm1_x = landmarks[lm1][0]\n",
    "    lm1_y = landmarks[lm1][1]\n",
    "    \n",
    "    lm2_x = landmarks[lm2][0]\n",
    "    lm2_y = landmarks[lm2][1]\n",
    "    \n",
    "    # Bestimmung der x-Koordinate des Mittelpunktes zwischen C. lacrimalis rechts und links\n",
    "    # ipp2_x_px: x-Koordinate der C. lacrimalis rechts (landmark 362) [Pixel]\n",
    "    # ipp1_x_px: x-Koordinate der C. lacrimalis links (landmark 133)  [Pixel]\n",
    "    # ipp_half: x-Koordinate der C. lacrimalis links + die Hälfte der Distanz zwischen lm362 und lm133 [Pixel]\n",
    "    #  - dies ist die x-Koordinate des Punktes, der später bei x = Bildbreite/2 liegen soll\n",
    "    ipp_half = lm1_x+(lm2_x-lm1_x)/2\n",
    "    \n",
    "    # Bestimmung der y-Koordinate des Punktes, der später bei y = Bildhöhe/2 liegen soll\n",
    "    if lm1_y < lm2_y:\n",
    "        ipp_half_2 = lm1_y+(lm2_y-lm1_y)/2\n",
    "    else:\n",
    "        ipp_half_2 = lm2_y+(lm1_y-lm2_y)/2\n",
    "    \n",
    "    # Bestimmung des Winkels zwischen der Linie zwischen C.l. sinister und dextra\n",
    "    dX = lm2_x - lm1_x\n",
    "    dY = lm2_y - lm2_y\n",
    "    angle = np.degrees(np.arctan2(dY, dX))\n",
    "    \n",
    "    # Offset = Differenz zwischen (x,y) des Bildzentrums und (x,y) der Landmark\n",
    "    #offsetX = (wi-ipp_half)\n",
    "    #offsetY = (he-ipp_half_2)\n",
    "    offsetX = (wi-ipp_half)\n",
    "    offsetY = (he-ipp_half_2)\n",
    "    \n",
    "    if suppress_output == False:\n",
    "        msg = f'''\n",
    "        EyeL x:   {lm1_x}\n",
    "        EyeR x:   {lm2_x}\n",
    "        EyeL y:   {lm1_y}\n",
    "        EyeR y:   {lm2_y}\n",
    "        IPP x:    {ipp_half}\n",
    "        IPP y:    {ipp_half_2}\n",
    "        Offset x: {offsetX}\n",
    "        Offset y: {offsetY}\n",
    "        Angle:    {angle}\n",
    "        \\n\\n\n",
    "        '''\n",
    "        print(msg)\n",
    "    \n",
    "    # Affine matrix with Translations\n",
    "    T = np.float32([[1, 0, offsetX], [0, 1, offsetY]]) \n",
    "    \n",
    "    # WarpAffine\n",
    "    centered_image = cv2.warpAffine(image, T, (width, height))\n",
    "    \n",
    "    # Return translated Image, x-Koordinate des Punktes zwischen C.l. sinistra und dextra, Winkel\n",
    "    return centered_image, ipp_half, ipp_half_2, angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47569797",
   "metadata": {},
   "source": [
    "#### iii) Bild zu Video Konvertierung\n",
    "\n",
    "Code is partially taken from:  \n",
    "[TheAILearner: openCV Img to Video](https://theailearner.com/2018/10/15/creating-video-from-images-using-opencv-python/)  \n",
    "[TheAILearner: Image Resizing, wenn Input und Video Size unterschiedlich](https://theailearner.com/2018/11/15/changing-video-resolution-using-opencv-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbdff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_video(image_path_list: list, target_folder: str, output_name: str, size:tuple, fps=20,):\n",
    "    \"\"\"\n",
    "    Converts a list of images to a video and saves it to the specified target folder with the given output name and file format.\n",
    "\n",
    "    Args:\n",
    "    image_path_list (list): A list of file paths of the images to be included in the video.\n",
    "    target_folder (str): The path to the directory where the video file will be saved.\n",
    "    output_name (str): The name of the video file to be saved.\n",
    "    size (tuple): A tuple of width and height values representing the size of the output video.\n",
    "    fps (int, optional): The frame rate of the output video. Default is 20 frames per second.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    img_array = []\n",
    "    \n",
    "    for filename in tqdm(image_path_list):\n",
    "        \n",
    "        # Read Image\n",
    "        img = cv2.imread(filename)\n",
    "\n",
    "        # Resize Image\n",
    "        img_resized = cv2.resize(img, \n",
    "                                 size, \n",
    "                                 fx=0,\n",
    "                                 fy=0, \n",
    "                                 interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "        # Append image to list \n",
    "        img_array.append(img_resized)\n",
    "\n",
    "    # Open video writer\n",
    "    out = cv2.VideoWriter(target_folder+os.sep+output_name+'.mp4',\n",
    "                              cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "                              fps, \n",
    "                              size)\n",
    "\n",
    "    # write images\n",
    "    for i in tqdm(range(len(img_array))):\n",
    "        out.write(img_array[i])\n",
    "\n",
    "    \n",
    "    # release video writer            \n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be007283",
   "metadata": {},
   "source": [
    "#### iv) Cropping und Resizing\n",
    "\n",
    "Code is partially taken from: `cutting.py` provided by [Prof. Dr. Gernot Horstmann](https://www.uni-bielefeld.de/fakultaeten/psychologie/abteilung/arbeitseinheiten/01/people/scientificstaff/horstmann/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92747ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_resize(input_folder: str, output_folder: str, output_width: int, output_height: int):\n",
    "    \"\"\"\n",
    "    Crop and resize images from a source folder and save them in a target folder.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): The path to the folder containing the source images.\n",
    "        output_folder (str): The path to the folder where the cropped and resized images will be saved.\n",
    "        output_width (int): The desired width of the output images, in pixels.\n",
    "        output_height (int): The desired height of the output images, in pixels.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "\n",
    "    Raises:\n",
    "        OSError: If the output folder cannot be created.\n",
    "\n",
    "    The function crops and resizes images from the input folder, using a fixed aspect ratio and centering the\n",
    "    cropping around the image center. The resulting images are saved in the output folder as JPEG files, with\n",
    "    the same base name as the source files.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Lese Bilder aus dem Verzeichnis: {input_folder}.')\n",
    "    print(f'Speichere cropped images in: {output_folder}.')\n",
    "        \n",
    "    \n",
    "    # Checking if the output folder, imgs_aligned, exists\n",
    "    # If it does not exist, it is created\n",
    "    if not os.path.exists(imgs_cropped):\n",
    "        os.makedirs(imgs_cropped)\n",
    "\n",
    "    # Get a list of all file names in the image directory\n",
    "    myImgList = os.listdir(input_folder)\n",
    "    myImgListEncode = [x.encode('utf-8') for x in myImgList]\n",
    "\n",
    "    # Iterate over file paths\n",
    "    for thisImg in tqdm(myImgList):\n",
    "\n",
    "        # open image\n",
    "        img =Image.open(os.path.join(input_folder, thisImg))\n",
    "\n",
    "        # Optional: Image Enhancer (i.e. brightness adjustments)\n",
    "        #enhancer=ImageEnhance.Brightness(img)\n",
    "        #img = enhancer.enhance(1.5)\n",
    "\n",
    "        # Center of input image\n",
    "        cenX = img.width//2\n",
    "        cenY = img.height//2\n",
    "\n",
    "        # Scaling Factor\n",
    "        f=50\n",
    "\n",
    "        # cropping of original image using scaling factor\n",
    "        cropped = img.crop((\n",
    "                cenX-(100*f),\n",
    "                cenY-(100*.682*f),\n",
    "                cenX+(100*f),\n",
    "                cenY+(100*.682*f)\n",
    "                ))\n",
    "\n",
    "        # Resizing\n",
    "        x_factor = y_factor = 0.45\n",
    "        img=cropped.resize( (int(img.size[0]*x_factor), int(img.size[1]*y_factor)), Image.ANTIALIAS)\n",
    "        img=cropped.resize( (int(img.size[0]*x_factor), int(img.size[1]*y_factor)), Resampling.LANCZOS)\n",
    "        \n",
    "        # Center of cropped image\n",
    "        cenX = img.width//2\n",
    "        cenY = img.height//2\n",
    "\n",
    "        # cropping of original image using desired output width and height\n",
    "        cropped = img.crop((\n",
    "                cenX-(output_width/2),\n",
    "                cenY-(output_height/2),\n",
    "                cenX+(output_width/2),\n",
    "                cenY+(output_height/2)\n",
    "                ))\n",
    "\n",
    "        # generate file name\n",
    "        imgName = os.path.splitext(thisImg)[0]\n",
    "        outName = imgName+\".jpg\"\n",
    "        outFile = os.path.join(output_folder, outName)\n",
    "\n",
    "        # save image to disk\n",
    "        cropped.save(outFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac5d5e",
   "metadata": {},
   "source": [
    "### d) Centering proper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa0fe7-cc56-4831-a6eb-5d2f4a1db356",
   "metadata": {},
   "source": [
    "#### i) Mediapipe Library (468 landmarks)\n",
    "\n",
    "__MediaPipe by Google__: [Github](https://github.com/google/mediapipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mp_drawing = mp.solutions.drawing_utils\n",
    "#mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Checking if the output folder, imgs_aligned, exists\n",
    "# If it does not exist, it is created\n",
    "if os.path.exists(imgs_aligned) == False:\n",
    "    os.mkdir(imgs_aligned)\n",
    "\n",
    "# Creating a list of absolute file paths for original images in folder imgs_orig\n",
    "IMAGE_FILES = glob.glob(imgs_orig+'\\\\*JPG')\n",
    "\n",
    "print(f'''There are {len(IMAGE_FILES)} images in folder {imgs_orig}.\\n\n",
    "Target folder set to: {imgs_aligned}''')\n",
    "\n",
    "# Initializing an empty list for angles between the left and right caruncula lacrimalis\n",
    "angles = []\n",
    "\n",
    "# Initializing an empty list for image names without file extension (looker ids)\n",
    "# This list will be used to merge the angle data with experimental data\n",
    "names = []\n",
    "\n",
    "# Initializing an empty list for file paths of images where no landmarks were found\n",
    "landmarks_not_found = []\n",
    "\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True, # individual images, not video streams\n",
    "    max_num_faces=1, # maximum number of faces (1 because only 1 face per image)\n",
    "    refine_landmarks=True, # should the mesh be refined around the eye region?\n",
    "    min_detection_confidence=0.5) as face_mesh:\n",
    "\n",
    "    # iterate over file paths\n",
    "    for file in tqdm(IMAGE_FILES):\n",
    "        \n",
    "        # read image\n",
    "        image = cv2.imread(file)\n",
    "        \n",
    "        # Generate filename (preserve original Filename)\n",
    "        img_path = Path(file)\n",
    "        name_without_ext = img_path.stem\n",
    "        name_with_ext = img_path.parts[-1]\n",
    "\n",
    "        # Convert the BGR image to RGB before processing.\n",
    "        # result contains 468 landmarks\n",
    "        results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # continue with next iteration if no landmark is found\n",
    "    # keep track of images (paths) with no detected landmarks\n",
    "        if not results.multi_face_landmarks:\n",
    "            print(f'Keine Landmarks gefunden in: {file}')\n",
    "            landmarks_not_found.append(file)\n",
    "            continue\n",
    "        \n",
    "        # center image using default parameters\n",
    "        centered_image, _, _, angle = center_image(image, results)\n",
    "        \n",
    "        # keep track of angle between line connecting lm1 and lm2 and x axis\n",
    "        angles.append(float(angle))\n",
    "        \n",
    "        # keep track of file names without extension\n",
    "        # This list will be used to merge the angle data with experimental data\n",
    "        names.append(name_without_ext)\n",
    "        \n",
    "        # write aligned / centered image to disk\n",
    "        cv2.imwrite(imgs_aligned+os.sep+name_with_ext , centered_image)\n",
    "\n",
    "# create and export data frame with angles and looker id\n",
    "df = pd.DataFrame({\"angles\":angles, \n",
    "                   \"looker\":names})\n",
    "df[\"angles\"] = round(df[\"angles\"], 3)\n",
    "df.to_csv(root+os.sep+'angles.csv', float_format=\"%.3f\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed096ed0-e440-4f93-b7d3-0cf896ddd92a",
   "metadata": {},
   "source": [
    "#### ii) mlxtend Library (68 Landmarks)\n",
    "\n",
    "__mlextend__: [GitHub](https://github.com/rasbt/mlxtend)\n",
    "\n",
    "__Code examples__ for mlxtend's `extract_face_landmarks`: [Sebastian Raschkas GitHub](https://rasbt.github.io/mlxtend/user_guide/image/extract_face_landmarks/#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11230873-2c7f-44d4-9d99-cd11bf74f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the output folder, imgs_aligned, exists\n",
    "# If it does not exist, it is created\n",
    "if os.path.exists(imgs_aligned) == False:\n",
    "    os.mkdir(imgs_aligned)\n",
    "\n",
    "# Creating a list of absolute file paths for original images in folder imgs_orig\n",
    "imgs = glob.glob(imgs_orig + \"\\*.JPG\")\n",
    "\n",
    "print(f'''There are {len(imgs)} images in folder {imgs_orig}.\\n\n",
    "Target folder set to: {imgs_aligned}''')\n",
    "\n",
    "# Initializing an empty list for angles between the left and right caruncula lacrimalis\n",
    "angles = []\n",
    "\n",
    "# Initializing an empty list for image names without file extension (looker ids)\n",
    "lookerIDs = []\n",
    "\n",
    "\n",
    "for file in tqdm(imgs):\n",
    "    \n",
    "    # read image\n",
    "    img = cv2.imread(file)\n",
    "    \n",
    "    # extract/detect landmarks\n",
    "    # landmark contains x- and y coordinates of the 68 extracte landmarks\n",
    "    landmarks = extract_face_landmarks(img)\n",
    "    \n",
    "    # enter image using default parameters\n",
    "    centered_image, _, _, angle = center_image_mlx(img, landmarks, suppress_output=True)\n",
    "    \n",
    "    # Generate filename (preserve original Filename)\n",
    "    img_path = Path(file)\n",
    "    name_without_ext = img_path.stem # Looker ID\n",
    "    name_with_ext = img_path.parts[-1] # Filename für zentriertes Bild\n",
    "    \n",
    "    # keep track of angle between line connecting lm1 and lm2 and x axis\n",
    "    angles.append(float(angle))\n",
    "\n",
    "    # keep track of file names without extension\n",
    "    # This list will be used to merge the angle data with experimental data\n",
    "    lookerIDs.append(name_without_ext)\n",
    "\n",
    "    # write aligned / centered image to disk\n",
    "    cv2.imwrite(imgs_aligned+os.sep+name_with_ext , centered_image)\n",
    "\n",
    "    \n",
    "# create and export data frame with angles and looker id\n",
    "df = pd.DataFrame({\"angles\":angles, \n",
    "                   \"looker\":lookerIDs})\n",
    "df[\"angles\"] = round(df[\"angles\"], 3)\n",
    "df.to_csv(root+os.sep+'angles.csv', float_format=\"%.3f\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f76dc",
   "metadata": {},
   "source": [
    "### e) (opt) Checking alignments by creating a video from original and aligned images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c9446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original images\n",
    "imgs_orig = glob.glob(imgs_orig+'\\\\*JPG')\n",
    "\n",
    "# Resized images\n",
    "imgs_aligned = glob.glob(imgs_aligned+'\\\\*jpg')\n",
    "\n",
    "# Size of output video file (x, y)\n",
    "size = (1920, 1080)\n",
    "\n",
    "# Create Video of Original jpg files\n",
    "img_to_video(imgs_orig,\n",
    "             root,\n",
    "            'Original_Images',\n",
    "            size)\n",
    "\n",
    "# Create Video of aligned jpg files\n",
    "img_to_video(imgs_aligned,\n",
    "             root,\n",
    "            'Aligned_Images',\n",
    "            size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdaad5b",
   "metadata": {},
   "source": [
    "# 2. Image Cropping and Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a24048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_resize(imgs_aligned_caf, # folder containing aligned and content aware filled images\n",
    "            imgs_cropped, # output folder for cropped images\n",
    "            1280, # desired output width\n",
    "            1024) # desired output height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ee04a",
   "metadata": {},
   "source": [
    "# 3. Create Excel file with paths to experimental stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f405ee0-ad0f-4368-af55-d960c6635942",
   "metadata": {},
   "source": [
    "### a) Data Frame with absolute and relative paths, looker ids and (corrected) gaze deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ad7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to create a training conditions file for the experiment.\n",
    "\n",
    "# gets the paths of training images and experimental images\n",
    "train_img_paths = glob.glob(exp + os.sep + \"img_train\" + os.sep + \"*.jpg\")\n",
    "exp_img_paths = glob.glob(exp + os.sep + \"img_exp\" + os.sep + \"*.jpg\")\n",
    "\n",
    "# Store the paths of all the images in a list\n",
    "all_paths = [train_img_paths, exp_img_paths]\n",
    "\n",
    "# create training conditions file\n",
    "for i, l in enumerate(all_paths):\n",
    "    \n",
    "    # # Initialize lists for Looker-ID, gaze direction in °Sehwinkeln, relative image paths and corrected visual angle\n",
    "    looker = []\n",
    "    visAng = []\n",
    "    rel_path = []\n",
    "    visAng_corr = []\n",
    "\n",
    "    \n",
    "    # Iterate over image paths in corresponding list \"l\"\n",
    "    for paths in l:\n",
    "\n",
    "        # Split paths at each os sep, underscore, and dot\n",
    "        split_parts = re.split(r'[\\\\/_.]', paths)\n",
    "\n",
    "        # Add looker id to looker list\n",
    "        looker.append(split_parts[-3])\n",
    "        \n",
    "        # Add visual angle to visAng list\n",
    "        va = int(split_parts[-2])\n",
    "        visAng.append(va)\n",
    "        \n",
    "        # Trainingsdaten (If statement checks if it is training data)\n",
    "        if i == 1:\n",
    "            # Korrektur des Sehwinkels\n",
    "            if va == 0:\n",
    "                visAng_corr.append(0)\n",
    "            elif va == 1:\n",
    "                visAng_corr.append(1.1)\n",
    "            elif va == 2:\n",
    "                visAng_corr.append(2.2)\n",
    "            elif va == 3:\n",
    "                visAng_corr.append(3.3)\n",
    "            elif va == 4:\n",
    "                visAng_corr.append(4.4)\n",
    "            elif va == 5:\n",
    "                visAng_corr.append(5.5)\n",
    "            elif va == 6:\n",
    "                visAng_corr.append(6.6)\n",
    "            elif va == 7:\n",
    "                visAng_corr.append(7.7)\n",
    "            elif va == 8:\n",
    "                visAng_corr.append(8.8)\n",
    "            elif va == 9:\n",
    "                visAng_corr.append(9.9)\n",
    "            elif va == 10:\n",
    "                visAng_corr.append(11.1)\n",
    "            elif va == 11:\n",
    "                visAng_corr.append(12.2)\n",
    "            elif va == 12:\n",
    "                visAng_corr.append(13.3)\n",
    "\n",
    "        # Add relative path of image file to list\n",
    "        rel_path.append(os.sep.join(paths.split(os.sep)[-2:]))\n",
    "    \n",
    "    # Create a DataFrame for training data or experimental data\n",
    "    if i == 0:\n",
    "        df_train = pd.DataFrame({'img_path': train_img_paths, \n",
    "                             'img_rel_path': rel_path, \n",
    "                             'looker': looker, \n",
    "                             'visAng': visAng})\n",
    "    else:\n",
    "        df_exp = pd.DataFrame({'img_path': exp_img_paths, \n",
    "                               'img_rel_path': rel_path, \n",
    "                               'looker': looker, \n",
    "                               'visAng': visAng,\n",
    "                               'visAngCorr': visAng_corr})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99859b3-1ef9-4fb5-820d-8bcaff7e9665",
   "metadata": {},
   "source": [
    "### b) Export data frames as condition files (xlsx) for PsychoPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export training condition file (df_train) as xlsx\n",
    "#df_train.to_excel(exp+os.sep+\"cond_training.xlsx\", index=False)       \n",
    "\n",
    "# List for visual angles used in the experiment\n",
    "visAng_exp = [0.0, 2.2, 4.4, 6.6, 8.8, 13.3]\n",
    "\n",
    "# List for visual angles not used in the experiment\n",
    "visAng_irrelevant = [1.1, 3.3, 5.5, 7.7, 9.9, 11.1, 12.2]\n",
    "\n",
    "# filter df for used stimuli\n",
    "filtered_df = df_exp[df_exp.visAngCorr.isin(visAng_exp)].copy()\n",
    "\n",
    "# filter df for unused stimuli\n",
    "filtered_df_nonrelevant_stimuli = df_exp[df_exp.visAngCorr.isin(visAng_irrelevant)]\n",
    "\n",
    "filtered_df[\"visAng\"] = filtered_df[\"visAng\"].astype('float64')\n",
    "filtered_df[\"visAngCorr\"] = filtered_df[\"visAngCorr\"].astype('float64')\n",
    "\n",
    "# write filtered df to disk\n",
    "filtered_df.to_excel(exp+os.sep+\"cond_exp.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d671a7-ed43-4e03-83b9-1c5085b3bc71",
   "metadata": {},
   "source": [
    "### c) Remove unused stimuli from experiment stimuli folder\n",
    "\n",
    "As the experiment includes a resource manager component that preloads all images in the relative directories img_train and img_exp, removing the unneeded images is resource-saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bccae-3822-498b-aded-1882f78a835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused image files from experiment dir (PsychoPy experiment root folder)\n",
    "for path in filtered_df_nonrelevant_stimuli[\"img_path\"].tolist():\n",
    "    os.remove(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
